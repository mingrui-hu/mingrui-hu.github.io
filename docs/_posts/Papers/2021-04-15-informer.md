---
layout: post
title:  "Informer notes"
categories: paper
---
# Informer

## Overall Network Structure

![Network](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/Fig2.png)

###  Inspiration Fact: Query Sparsity 

- Long tail distribution of self-attention weights
- ![LongTailDistribution](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/Fig7.png)

### Scenario: Long Sequence Time-series Forecasting(LSTF)

- horizon > 300

- Problem Setting:

  ![ProblemSetting](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/LSTF.png)

#### Target:

1. reduce attention operation speed 

   $O(L^2) -> O(L*logL)$

2. reduce memory usage per attention layer 

   O(L^2) -> O(LlogL)$

## Improvements

#### 1. ProbSparse Self-attention

1.1 **Query sparse measurements**

- the i_th query's attention on all keys:

$p(k_j|q_i) = \frac{k(q_i, k_j)}{\Sigma_lk(q_i, k_l)}$ ,

where k = asymmetric exponetial kernel $exp(q_ik_j^T/\sqrt{d})$

- $q(k_j|q_i) = 1/L_k$

- $KL(q||p) = ln\Sigma_{l=1}^{L_k} e^{q_ik_i^T/\sqrt{d}} - \frac{1}{L_k}\Sigma_{l=1}^{L_k}q_ik_i^T/\sqrt{d} - lnL_k$

- the i_th query's sparsity measurements:

  $\bold{M}(q_i, K) = ln\Sigma_{l=1}^{L_k} e^{\frac{q_ik_i^T}{\sqrt{d}}} - \frac{1}{L_k}\Sigma_{l=1}^{L_k}\frac{q_ik_i^T}{\sqrt{d}}$

1.2 **ProbSparse Self-attention**

$A(Q, K, V) = Softmax(\frac{\overline{Q}K^T}{\sqrt{d}}V)$

where Q contains **top-u** queries under **M**

1.3 **Sampling** 

- $u = c *lnL_Q$ , where c is a constant sampling factor

- to obtain top-u queries in $O(LlnL)$:

  - Approximation: max-mean measurements
    - $\overline{M}(q_i, K) = max_j\{\frac{q_ik_j^T}{\sqrt{d}}\} -  \frac{1}{L_k}\Sigma_{l=1}^{L_k}\frac{q_ik_i^T}{\sqrt{d}}$
  - Prove: the range of top-u holds in the boundary relaxation
  - *???* **then -> Under the long tail distribution: randomly sample $U = L_klnL_Q$** to calculate $\overline{M}(q_i, K)$ (fill others with zero)

  ![PesuedoCode](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/ProbSparsePseudoCode.png)

#### 2. Self-attention Distilling

![Distill](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/Fig3.png)

$X_{t=j+1}^t = MaxPool(ELU(Conv1d[X^t_j]_{AB}))$

reduce memory to $O((2-\epsilon)LlogL)$

#### 3. Generative Decoder

- Start Token:
  - $X_{de}^t = Concat(X_{token}^t, X_0^t) \in \mathbb{R}^{(L_{token} + L_y) \times d_{model}}$

- Pro: 
  - Non-autoregressive
  - Only one forward pass
  - Allow inputs with timestamps/offsets

- Limits: 
  - Pre-Fixed Length Output Sequence
  - Start Token: previous known labels





#### Appendix:

![NetComponents](/Users/humingrui/REPO/mingrui-hu.github.io/docs/_posts/Papers/informer_figs/NetworkComponents.png)





